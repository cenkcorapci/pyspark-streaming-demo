{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lined-gateway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.11\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offshore-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worth-evidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Caskroom/miniconda/base/envs/anchormen/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.11-2.4.7.jar) to method java.nio.Bits.unaligned()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/28 22:20:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName=SPARK_APP_NAME)\n",
    "sc.setLogLevel('ERROR')  # ignore INFO\n",
    "ssc = StreamingContext(sc, BATCH_DURATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "________________________________________________________________________________________________\n",
      "\n",
      "  Spark Streaming's Kafka libraries not found in class path. Try one of the following.\n",
      "\n",
      "  1. Include the Kafka library and its dependencies with in the\n",
      "     spark-submit command as\n",
      "\n",
      "     $ bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8:2.4.7 ...\n",
      "\n",
      "  2. Download the JAR of the artifact from Maven Central http://search.maven.org/,\n",
      "     Group Id = org.apache.spark, Artifact Id = spark-streaming-kafka-0-8-assembly, Version = 2.4.7.\n",
      "     Then, include the jar in the spark-submit command as\n",
      "\n",
      "     $ bin/spark-submit --jars <spark-streaming-kafka-0-8-assembly.jar> ...\n",
      "\n",
      "________________________________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/kx/0_ld99mx1nqglmt1c5g4dl6c0000gn/T/ipykernel_15148/488145159.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m ks = KafkaUtils.createStream(ssc, ZOOKEEPER_ADDRESS,\n\u001B[1;32m      2\u001B[0m                              \u001B[0;34m'spark-streaming-consumer'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m                              {KAFKA_TOPIC: 1})  # 1 meaning partition size\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniconda/base/envs/anchormen/lib/python3.7/site-packages/pyspark/streaming/kafka.py\u001B[0m in \u001B[0;36mcreateStream\u001B[0;34m(ssc, zkQuorum, groupId, topics, kafkaParams, storageLevel, keyDecoder, valueDecoder)\u001B[0m\n\u001B[1;32m     76\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"topics should be dict\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m         \u001B[0mjlevel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mssc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getJavaStorageLevel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstorageLevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 78\u001B[0;31m         \u001B[0mhelper\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mKafkaUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_helper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mssc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     79\u001B[0m         \u001B[0mjstream\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhelper\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateStream\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mssc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jssc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkafkaParams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtopics\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mjlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     80\u001B[0m         \u001B[0mser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPairDeserializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mNoOpSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mNoOpSerializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniconda/base/envs/anchormen/lib/python3.7/site-packages/pyspark/streaming/kafka.py\u001B[0m in \u001B[0;36m_get_helper\u001B[0;34m(sc)\u001B[0m\n\u001B[1;32m    215\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_get_helper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    216\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 217\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstreaming\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkafka\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mKafkaUtilsPythonHelper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    218\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    219\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"'JavaPackage' object is not callable\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "ks = KafkaUtils.createStream(ssc, ZOOKEEPER_ADDRESS,\n",
    "                             'spark-streaming-consumer',\n",
    "                             {KAFKA_TOPIC: 1})  # 1 meaning partition size\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}